#!/bin/bash
# Quick fix untuk masalah 2 menit response time

echo "üöÄ Quick Fix: Optimizing Chatbot Performance"
echo "=============================================="
echo ""

# 1. Check Ollama model
echo "1Ô∏è‚É£ Checking Ollama model..."
CURRENT_MODEL=$(grep "OLLAMA_MODEL" .env 2>/dev/null | cut -d'=' -f2)
echo "   Current model: $CURRENT_MODEL"

if [[ "$CURRENT_MODEL" == *"mistral"* ]] && [[ "$CURRENT_MODEL" != *"q4"* ]]; then
    echo "   ‚ö†Ô∏è  WARNING: Using full-size model (slow!)"
    echo "   Recommendation: Use quantized model for 3-5x speedup"
    echo ""
    echo "   Run this to switch to faster model:"
    echo "   ollama pull mistral:7b-instruct-q4_0"
    echo "   Then update .env: OLLAMA_MODEL=mistral:7b-instruct-q4_0"
fi

echo ""

# 2. Add HNSW index
echo "2Ô∏è‚É£ Adding HNSW index for faster vector search..."
python scripts/add_vector_index.py
echo ""

# 3. Restart app
echo "3Ô∏è‚É£ Changes applied to LLM service:"
echo "   ‚úì Timeout reduced: 25s ‚Üí 10s (fail fast)"
echo "   ‚úì Max tokens reduced: 200 ‚Üí 100 (faster generation)"
echo "   ‚úì Context window reduced: 1024 ‚Üí 512 (less processing)"
echo "   ‚úì Chunks reduced: 3 ‚Üí 2 (less context to process)"
echo ""

echo "4Ô∏è‚É£ Next steps:"
echo "   1. Restart your FastAPI server"
echo "   2. Test with a query"
echo "   3. Run: python scripts/diagnose_performance.py"
echo ""

echo "‚úÖ Quick fix applied!"
echo ""
echo "Expected improvements:"
echo "   - LLM timeout at 10s (no more 2-minute waits)"
echo "   - Faster generation with reduced tokens"
echo "   - Fallback response if LLM is too slow"
